# %% å‚æ•°ç»Ÿè®¡è„šæœ¬ï¼šç»Ÿè®¡æ•´ä¸ª FluxPipeline é‡Œæ‰€æœ‰å‚æ•°ï¼ˆæ‰€æœ‰æ¨¡å—ã€æ‰€æœ‰æƒé‡ï¼‰
import torch
from src.pipeline_pe_clone import FluxPipeline


def _get_dtype(name: str):
    name = (name or "").lower()
    if name in ["bf16", "bfloat16"]:
        return torch.bfloat16
    if name in ["fp16", "float16", "half"]:
        return torch.float16
    return torch.float32


# ==== ä¿®æ”¹æˆä½ è‡ªå·±çš„é…ç½® ====
MODEL_PATH = "black-forest-labs/FLUX.1-dev"
LORA_PATH = "/root/PhotoDoodle/outputs/rt1_clean/checkpoint-58000/pytorch_lora_weights.safetensors"
DTYPE = "bfloat16"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ==== æ”¶é›†æ¨¡å— & å‚æ•°è®¡æ•°å‡½æ•° ====
def get_unique_modules_from_pipeline(pipeline: FluxPipeline):
    """
    ä»Ž pipeline.components æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„ nn.Moduleï¼Œ
    é¿å…é‡å¤ç»Ÿè®¡å…±äº«æ¨¡å—ã€‚
    """
    modules = []
    seen = set()

    # Diffusers é£Žæ ¼ï¼špipeline.components æ˜¯ä¸€ä¸ª dict
    if hasattr(pipeline, "components"):
        for name, comp in pipeline.components.items():
            if isinstance(comp, torch.nn.Module):
                if id(comp) not in seen:
                    modules.append((name, comp))
                    seen.add(id(comp))
    else:
        # å…œåº•ï¼šå¦‚æžœæ²¡æœ‰ componentsï¼Œå°±å°è¯•ç›´æŽ¥åœ¨å±žæ€§é‡Œæ‰¾ nn.Module
        for name in dir(pipeline):
            if name.startswith("_"):
                continue
            try:
                attr = getattr(pipeline, name)
            except Exception:
                continue
            if isinstance(attr, torch.nn.Module):
                if id(attr) not in seen:
                    modules.append((name, attr))
                    seen.add(id(attr))

    return modules


def count_params_in_modules(modules):
    total = 0
    trainable = 0
    for name, module in modules:
        for p in module.parameters():
            n = p.numel()
            total += n
            if p.requires_grad:
                trainable += n
    return total, trainable


def count_params_by_module(modules):
    stats = {}
    for name, module in modules:
        t = 0
        tr = 0
        for p in module.parameters():
            n = p.numel()
            t += n
            if p.requires_grad:
                tr += n
        stats[name] = {"total": t, "trainable": tr}
    return stats


def count_lora_params_in_modules(modules):
    lora_total = 0
    for name, module in modules:
        for n, p in module.named_parameters():
            full_name = f"{name}.{n}"
            if "lora" in full_name.lower():
                lora_total += p.numel()
    return lora_total


# ==== pipeline åŠ è½½ ====
print("\n[INFO] Loading pipeline...")
pipe = FluxPipeline.from_pretrained(
    MODEL_PATH,
    torch_dtype=_get_dtype(DTYPE),
    local_files_only=True
).to(DEVICE)

print(f"[INFO] Loading LoRA: {LORA_PATH}")
pipe.load_lora_weights(LORA_PATH)

# ==== æ”¶é›†æ‰€æœ‰æ¨¡å— ====
modules = get_unique_modules_from_pipeline(pipe)

print("\n[INFO] Found the following nn.Modules in pipeline:")
for name, _ in modules:
    print(f"  - {name}")

# ==== å…¨æ¨¡åž‹å‚æ•° ====
total, trainable = count_params_in_modules(modules)

print("\n==============================")
print("ðŸ“¦ Total Parameters in Pipeline")
print("==============================")
print(f"Total params     : {total:,}")
print(f"Trainable params : {trainable:,}")
print(f"Non-trainable    : {total - trainable:,}")

# æ ¹æ® dtype ä¼°ç®—æ˜¾å­˜å ç”¨
bytes_per_param = 2 if DTYPE in ["bfloat16", "float16", "fp16"] else 4
gb = total * bytes_per_param / 1024**3
print(f"\nEstimated model size in {DTYPE}: {gb:.2f} GB\n")


# ==== æ¯ä¸ªå­æ¨¡å—å‚æ•° ====
print("===================================")
print("ðŸ“Œ Parameters by Submodule (Top-level components)")
print("===================================\n")

stats = count_params_by_module(modules)
for name, s in stats.items():
    t, tr = s["total"], s["trainable"]
    print(f"{name:30s} | total: {t:12,} | trainable: {tr:12,}")


# ==== LoRA å‚æ•°å•ç‹¬ç»Ÿè®¡ ====
lora_params = count_lora_params_in_modules(modules)
print("\n==============================")
print("ðŸŽ¯ LoRA Parameters Count")
print("==============================")
print(f"LoRA params: {lora_params:,}")
if total > 0:
    print(f"LoRA ratio: {lora_params/total*100:.4f}% of total params\n")
else:
    print()
