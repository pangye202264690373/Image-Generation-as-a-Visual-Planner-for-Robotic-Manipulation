{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # üîç Evaluate 3√ó3 Grid Image Quality\n",
    "# \n",
    "# This notebook compares generated 3√ó3 grid images against their GT grids.\n",
    "# \n",
    "# ‚úÖ Supported modes:\n",
    "# - **strict** ‚Üí skip if GT/GEN sizes differ  \n",
    "# - **resize_gen_to_gt** ‚Üí resize GEN to GT's size before cutting  \n",
    "# \n",
    "# Output metrics: FVD, SSIM, PSNR, LPIPS, MSE\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== ‚¨áÔ∏è ÈÖçÁΩÆÂå∫ ====\n",
    "CONFIG = {\n",
    "    \"jsonl_path\": \"/root/PhotoDoodle/data/bridge_test/text_test_index.jsonl\",  # ÂéüÂßã JSONL\n",
    "    \"output_dir\": \"inference/outputs_bridgeV2_text\",                           # Êé®ÁêÜÁªìÊûúÊñá‰ª∂Â§π\n",
    "    \"fit_mode\": \"resize_gen_to_gt\",    # \"strict\" Êàñ \"resize_gen_to_gt\"\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"only_final\": False                # Ëã• TrueÔºåÂàôÂè™ÊØîËæÉÊúÄÂêé‰∏ÄÂ∏ß\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ÂØºÂÖ•‰Ω†Â∑≤ÊúâÁöÑÊåáÊ†áÂáΩÊï∞ ===\n",
    "from calculate_fvd import calculate_fvd\n",
    "from calculate_psnr import calculate_psnr\n",
    "from calculate_ssim import calculate_ssim\n",
    "from calculate_lpips import calculate_lpips\n",
    "from calculate_mse import calculate_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ==== Â∑•ÂÖ∑ÂáΩÊï∞ ====\n",
    "def resolve_path_maybe_relative(p: str, base: Path) -> Path:\n",
    "    pp = Path(p)\n",
    "    return pp if pp.is_absolute() else (base / pp).resolve()\n",
    "\n",
    "def _assert_divisible_by_3(W: int, H: int, tag: str, path: Path):\n",
    "    assert W % 3 == 0 and H % 3 == 0, f\"[{tag}] Grid size not divisible by 3: {path} ({W}x{H})\"\n",
    "\n",
    "def _cut_3x3_to_video(im: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"Áõ¥Êé•Êåâ3√ó3ÂàáÂâ≤ÔºåÈ°∫Â∫è‰∏∫ 123 / 654 / 789\"\"\"\n",
    "    W, H = im.size\n",
    "    _assert_divisible_by_3(W, H, \"CUT\", Path(\"<in-memory>\"))\n",
    "    tile_w, tile_h = W // 3, H // 3\n",
    "    boxes = [(c*tile_w, r*tile_h, (c+1)*tile_w, (r+1)*tile_h)\n",
    "             for r in range(3) for c in range(3)]\n",
    "    order = [0, 1, 2, 5, 4, 3, 6, 7, 8]\n",
    "\n",
    "    frames = []\n",
    "    for idx in order:\n",
    "        patch = im.crop(boxes[idx])\n",
    "        arr = np.asarray(patch, dtype=np.uint8)\n",
    "        ten = torch.from_numpy(arr).permute(2, 0, 1).float() / 255.0\n",
    "        frames.append(ten)\n",
    "    return torch.stack(frames, dim=0)  # [9,3,H,W]\n",
    "\n",
    "def cut_grid_as_video_with_fit(gt_path: Path, gen_path: Path, fit_mode: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    gt_im = Image.open(gt_path).convert(\"RGB\")\n",
    "    Wg, Hg = gt_im.size\n",
    "    _assert_divisible_by_3(Wg, Hg, \"GT\", gt_path)\n",
    "\n",
    "    gen_im = Image.open(gen_path).convert(\"RGB\")\n",
    "    Wp, Hp = gen_im.size\n",
    "    _assert_divisible_by_3(Wp, Hp, \"GEN\", gen_path)\n",
    "\n",
    "    if fit_mode == \"strict\":\n",
    "        assert (Wp, Hp) == (Wg, Hg), f\"size mismatch GT({Wg}x{Hg}) vs GEN({Wp}x{Hp})\"\n",
    "    elif fit_mode == \"resize_gen_to_gt\" and (Wp, Hp) != (Wg, Hg):\n",
    "        gen_im = gen_im.resize((Wg, Hg), Image.BICUBIC)\n",
    "\n",
    "    v_gt, v_gen = _cut_3x3_to_video(gt_im), _cut_3x3_to_video(gen_im)\n",
    "    gt_im.close(); gen_im.close()\n",
    "    return v_gt, v_gen\n",
    "\n",
    "# %%\n",
    "# ==== Âä†ËΩΩÊ†∑Êú¨ÂØπ ====\n",
    "cfg = CONFIG\n",
    "JSONL_PATH   = Path(cfg[\"jsonl_path\"]).resolve()\n",
    "JSONL_BASE   = JSONL_PATH.parent\n",
    "OUTPUT_DIR   = Path(cfg[\"output_dir\"]).resolve()\n",
    "RESULTS_PATH = OUTPUT_DIR / \"results.jsonl\"\n",
    "\n",
    "pairs: List[Tuple[Path, Path, dict]] = []\n",
    "n_total = 0\n",
    "n_ok = 0\n",
    "\n",
    "assert RESULTS_PATH.exists(), f\"results.jsonl not found: {RESULTS_PATH}\"\n",
    "with open(RESULTS_PATH, \"r\", encoding=\"utf-8\") as fr:\n",
    "    for line in fr:\n",
    "        n_total += 1\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not rec.get(\"ok\", False):\n",
    "            continue\n",
    "\n",
    "        out_rel = rec.get(\"output\")\n",
    "        if not out_rel:\n",
    "            continue\n",
    "        gen_path = (OUTPUT_DIR / out_rel).resolve()\n",
    "\n",
    "        gt_raw = rec.get(\"target\", None) or rec.get(\"gt\", None)\n",
    "        if not gt_raw:\n",
    "            continue\n",
    "        gt_path = resolve_path_maybe_relative(gt_raw, JSONL_BASE)\n",
    "\n",
    "        if not gen_path.exists() or not gt_path.exists():\n",
    "            continue\n",
    "\n",
    "        pairs.append((gt_path, gen_path, rec))\n",
    "        n_ok += 1\n",
    "\n",
    "print(f\"[PAIRING] total_lines={n_total}, usable_pairs={n_ok}\")\n",
    "\n",
    "# %%\n",
    "# ==== ÊûÑÂª∫ËßÜÈ¢ëÂº†Èáè ====\n",
    "videos_gt_list, videos_gen_list = [], []\n",
    "FIT_MODE = cfg[\"fit_mode\"]\n",
    "\n",
    "for gt_path, gen_path, rec in pairs:\n",
    "    try:\n",
    "        v_gt, v_gen = cut_grid_as_video_with_fit(gt_path, gen_path, FIT_MODE)\n",
    "        if v_gt.shape != v_gen.shape:\n",
    "            continue\n",
    "        videos_gt_list.append(v_gt)\n",
    "        videos_gen_list.append(v_gen)\n",
    "    except Exception as e:\n",
    "        # print(f\"[SKIP] {e}\")\n",
    "        continue\n",
    "\n",
    "assert len(videos_gt_list) > 0, \"No valid pairs after processing.\"\n",
    "\n",
    "videos_gt  = torch.stack(videos_gt_list,  dim=0)\n",
    "videos_gen = torch.stack(videos_gen_list, dim=0)\n",
    "NUMBER_OF_VIDEOS, VIDEO_LENGTH, CHANNEL, H, W = videos_gt.shape\n",
    "print(f\"[DATA] videos_gt={videos_gt.shape}, videos_gen={videos_gen.shape}, FIT_MODE={FIT_MODE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69cd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as pyjson\n",
    "from datetime import datetime\n",
    "\n",
    "DEVICE = torch.device(cfg[\"device\"])\n",
    "ONLY_FINAL = cfg[\"only_final\"]\n",
    "\n",
    "# GPU tensors for FVD / LPIPS\n",
    "v1_gpu = videos_gt.to(DEVICE, non_blocking=True)\n",
    "v2_gpu = videos_gen.to(DEVICE, non_blocking=True)\n",
    "\n",
    "# CPU tensors for SSIM / PSNR / MSE (their implementations call .numpy())\n",
    "v1_cpu = videos_gt.cpu()\n",
    "v2_cpu = videos_gen.cpu()\n",
    "\n",
    "# ---- compute ----\n",
    "metrics = {}\n",
    "# GPU-heavy\n",
    "metrics[\"fvd\"]   = float(calculate_fvd(v1_gpu, v2_gpu, DEVICE, method='styleganv', only_final=ONLY_FINAL))\n",
    "metrics[\"lpips\"] = float(calculate_lpips(v1_gpu, v2_gpu, DEVICE, only_final=ONLY_FINAL))\n",
    "# CPU / numpy\n",
    "metrics[\"ssim\"]  = float(calculate_ssim(v1_cpu, v2_cpu, only_final=ONLY_FINAL))\n",
    "metrics[\"psnr\"]  = float(calculate_psnr(v1_cpu, v2_cpu, only_final=ONLY_FINAL))\n",
    "metrics[\"mse\"]   = float(calculate_mse(v1_cpu, v2_cpu, only_final=ONLY_FINAL))\n",
    "\n",
    "# ---- meta (flattened) ----\n",
    "N, T, C, H, W = videos_gt.shape\n",
    "flat_record = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"fit_mode\": str(FIT_MODE),\n",
    "    \"only_final\": bool(ONLY_FINAL),\n",
    "    \"num_pairs\": int(N),\n",
    "    \"video_length\": int(T),\n",
    "    \"channels\": int(C),\n",
    "    \"height\": int(H),\n",
    "    \"width\": int(W),\n",
    "    # metrics flattened:\n",
    "    **metrics,\n",
    "}\n",
    "\n",
    "print(pyjson.dumps(flat_record, indent=2))\n",
    "\n",
    "# ---- save flattened json ----\n",
    "metrics_path = OUTPUT_DIR / f\"metrics_fit_{FIT_MODE}_flat.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(pyjson.dumps(flat_record, indent=2))\n",
    "print(f\"[OK] flattened metrics saved to: {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
