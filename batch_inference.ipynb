{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47199b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b38b4",
   "metadata": {},
   "source": [
    "## jocoplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4851c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== 直接在这里指定配置 ====\n",
    "# CONFIG = {\n",
    "#     \"model_path\": \"black-forest-labs/FLUX.1-dev\",         # 本地 FLUX 基础模型目录\n",
    "#     \"lora_path\": \"/root/PhotoDoodle/outputs/jocoplay/checkpoint-50000/pytorch_lora_weights.safetensors\", \n",
    "#     # ↑ 直接换成你要使用的本地 LoRA 权重，例如:\n",
    "#     # \"/home/u12027/PhotoDoodle_LoRA/sksmagiceffects.safetensors\"\n",
    "    \n",
    "#     \"jsonl_path\": \"/root/PhotoDoodle/data/joco_merge_clean_fulltext_test/text_aug_rand.jsonl\",                # JSONL 输入文件（每行: {\"text\": \"...\", \"source\": \"/path/to/image\"}）\n",
    "#     \"output_dir\": \"inference/outputs_joco_text_2\",                    # 输出目录\n",
    "#     \"height\": 672,\n",
    "#     \"width\": 672,\n",
    "#     \"guidance_scale\": 3.5,\n",
    "#     \"num_steps\": 20,\n",
    "#     \"max_sequence_length\": 512,\n",
    "#     \"dtype\": \"bfloat16\",                              # \"bfloat16\" / \"float16\" / \"float32\"\n",
    "#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     \"seed\": None                                      # 固定随机种子(如 1234)，None 则随机\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c36a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== 直接在这里指定配置 ====\n",
    "# CONFIG = {\n",
    "#     \"model_path\": \"black-forest-labs/FLUX.1-dev\",         # 本地 FLUX 基础模型目录\n",
    "#     \"lora_path\": \"/root/PhotoDoodle/outputs/jocoplay_traj/checkpoint-50000/pytorch_lora_weights.safetensors\", \n",
    "#     # ↑ 直接换成你要使用的本地 LoRA 权重，例如:\n",
    "#     # \"/home/u12027/PhotoDoodle_LoRA/sksmagiceffects.safetensors\"\n",
    "    \n",
    "#     \"jsonl_path\": \"/root/PhotoDoodle/data/joco_merge_traj_test/pairs_text_inpaint.jsonl\",                # JSONL 输入文件（每行: {\"text\": \"...\", \"source\": \"/path/to/image\"}）\n",
    "#     \"output_dir\": \"inference/outputs_joco_traj_2\",                    # 输出目录\n",
    "#     \"height\": 672,\n",
    "#     \"width\": 672,\n",
    "#     \"guidance_scale\": 3.5,\n",
    "#     \"num_steps\": 20,\n",
    "#     \"max_sequence_length\": 512,\n",
    "#     \"dtype\": \"bfloat16\",                              # \"bfloat16\" / \"float16\" / \"float32\"\n",
    "#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     \"seed\": None                                      # 固定随机种子(如 1234)，None 则随机\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89d2c2",
   "metadata": {},
   "source": [
    "## bridge V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930fc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 直接在这里指定配置 ====\n",
    "CONFIG = {\n",
    "    \"model_path\": \"black-forest-labs/FLUX.1-dev\",         # 本地 FLUX 基础模型目录\n",
    "    \"lora_path\": \"/root/PhotoDoodle/outputs/bridge_clean/checkpoint-104000/pytorch_lora_weights.safetensors\", \n",
    "    # ↑ 直接换成你要使用的本地 LoRA 权重，例如:\n",
    "    # \"/home/u12027/PhotoDoodle_LoRA/sksmagiceffects.safetensors\"\n",
    "    \n",
    "    \"jsonl_path\": \"/root/PhotoDoodle/data/bridge_test/text_test_index_noop.jsonl\",                # JSONL 输入文件（每行: {\"text\": \"...\", \"source\": \"/path/to/image\"}）\n",
    "    \"output_dir\": \"inference/outputs_bridgeV2_text_notext\",                    # 输出目录\n",
    "    \"height\": 720,\n",
    "    \"width\":960,\n",
    "    \"guidance_scale\": 3.5,\n",
    "    \"num_steps\": 20,\n",
    "    \"max_sequence_length\": 512,\n",
    "    \"dtype\": \"bfloat16\",                              # \"bfloat16\" / \"float16\" / \"float32\"\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": None                                      # 固定随机种子(如 1234)，None 则随机\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== 直接在这里指定配置 ====\n",
    "# CONFIG = {\n",
    "#     \"model_path\": \"black-forest-labs/FLUX.1-dev\",         # 本地 FLUX 基础模型目录\n",
    "#     \"lora_path\": \"/root/PhotoDoodle/outputs/bridge_traj/checkpoint-104000/pytorch_lora_weights.safetensors\", \n",
    "#     # ↑ 直接换成你要使用的本地 LoRA 权重，例如:\n",
    "#     # \"/home/u12027/PhotoDoodle_LoRA/sksmagiceffects.safetensors\"\n",
    "    \n",
    "#     \"jsonl_path\": \"/root/PhotoDoodle/data/bridge_test/traj_test_index.jsonl\",                # JSONL 输入文件（每行: {\"text\": \"...\", \"source\": \"/path/to/image\"}）\n",
    "#     \"output_dir\": \"inference/outputs_bridgeV2_traj_notTrajectory\",                    # 输出目录\n",
    "#     \"height\": 720,\n",
    "#     \"width\":960,\n",
    "#     \"guidance_scale\": 3.5,\n",
    "#     \"num_steps\": 20,\n",
    "#     \"max_sequence_length\": 512,\n",
    "#     \"dtype\": \"bfloat16\",                              # \"bfloat16\" / \"float16\" / \"float32\"\n",
    "#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     \"seed\": None                                      # 固定随机种子(如 1234)，None 则随机\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f31eaf",
   "metadata": {},
   "source": [
    "## rt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b748769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== 直接在这里指定配置 ====\n",
    "# CONFIG = {\n",
    "#     \"model_path\": \"black-forest-labs/FLUX.1-dev\",         # 本地 FLUX 基础模型目录\n",
    "#     \"lora_path\": \"/root/PhotoDoodle/outputs/rt1_clean/checkpoint-58000/pytorch_lora_weights.safetensors\", \n",
    "    \n",
    "#     \"jsonl_path\": \"/root/PhotoDoodle/data/rt1_merge_clean/pairs_test_single_augmented_index.jsonl\",                # JSONL 输入文件（每行: {\"text\": \"...\", \"source\": \"/path/to/image\"}）\n",
    "#     \"output_dir\": \"inference/outputs_rt1_text\",                    # 输出目录\n",
    "#     \"height\": 768,\n",
    "#     \"width\":960,\n",
    "#     \"guidance_scale\": 3.5,\n",
    "#     \"num_steps\": 20,\n",
    "#     \"max_sequence_length\": 512,\n",
    "#     \"dtype\": \"bfloat16\",                              # \"bfloat16\" / \"float16\" / \"float32\"\n",
    "#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     \"seed\": None                                      # 固定随机种子(如 1234)，None 则随机\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65033819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== 直接在这里指定配置 ====\n",
    "# CONFIG = {\n",
    "#     \"model_path\": \"black-forest-labs/FLUX.1-dev\",         # 本地 FLUX 基础模型目录\n",
    "#     \"lora_path\": \"/root/PhotoDoodle/outputs/rt1_traj/checkpoint-58000/pytorch_lora_weights.safetensors\", \n",
    "\n",
    "#     \"jsonl_path\": \"/root/PhotoDoodle/data/rt1_merge_traj/pairs_test_traj_replaced.jsonl\",                # JSONL 输入文件（每行: {\"text\": \"...\", \"source\": \"/path/to/image\"}）\n",
    "#     \"output_dir\": \"inference/outputs_rt1_traj\",                    # 输出目录\n",
    "#     \"height\": 768,\n",
    "#     \"width\":960,\n",
    "#     \"guidance_scale\": 3.5,\n",
    "#     \"num_steps\": 20,\n",
    "#     \"max_sequence_length\": 512,\n",
    "#     \"dtype\": \"bfloat16\",                              # \"bfloat16\" / \"float16\" / \"float32\"\n",
    "#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     \"seed\": None                                      # 固定随机种子(如 1234)，None 则随机\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb5b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==== 导入 Pipeline ====\n",
    "from src.pipeline_pe_clone import FluxPipeline\n",
    "\n",
    "\n",
    "def _get_dtype(name: str):\n",
    "    n = (name or \"\").lower()\n",
    "    if n in [\"bf16\", \"bfloat16\"]:\n",
    "        return torch.bfloat16\n",
    "    if n in [\"fp16\", \"float16\", \"half\"]:\n",
    "        return torch.float16\n",
    "    return torch.float32\n",
    "\n",
    "\n",
    "def load_pipeline(model_path: str, lora_path: str, dtype: str, device: str) -> FluxPipeline:\n",
    "    \"\"\"只加载本地模型和本地 LoRA\"\"\"\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=_get_dtype(dtype),\n",
    "        local_files_only=True     # 不联网\n",
    "    ).to(device)\n",
    "\n",
    "    # === 第 ① 步：加载并 fuse 预训练 LoRA ===\n",
    "    print(f\"[INFO] Loading pretrain LoRA\")\n",
    "    pipe.load_lora_weights(\"nicolaus-huang/PhotoDoodle\", weight_name=\"pretrain.safetensors\", local_files_only=True)\n",
    "    pipe.fuse_lora()                 # 合并到 base\n",
    "    pipe.unload_lora_weights()       # 卸载 adapter 节省显存\n",
    "\n",
    "    # 直接加载指定 LoRA\n",
    "    if lora_path:\n",
    "        print(f\"[INFO] Loading LoRA: {lora_path}\")\n",
    "        pipe.load_lora_weights(lora_path)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def run_batch_inference(\n",
    "    pipeline: FluxPipeline,\n",
    "    jsonl_path: str,\n",
    "    output_dir: str,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    guidance_scale: float,\n",
    "    num_steps: int,\n",
    "    max_sequence_length: int,\n",
    "    seed: Optional[int],\n",
    "    device: str\n",
    "):\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "    base_dir = jsonl_path.parent\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_path = output_dir / \"results.jsonl\"\n",
    "\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    lines = jsonl_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    with open(results_path, \"w\", encoding=\"utf-8\") as fw:\n",
    "        for idx, line in enumerate(tqdm(lines, desc=\"Batch Inference\")):\n",
    "            # 解析 JSON\n",
    "            try:\n",
    "                rec: Dict[str, Any] = json.loads(line)\n",
    "            except Exception as e:\n",
    "                fw.write(json.dumps({\"idx\": idx, \"ok\": False, \"error\": f\"JSON error: {e}\", \"raw\": line}, ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            text = (rec.get(\"text\") or \"\").strip()\n",
    "            src = rec.get(\"source\")\n",
    "\n",
    "            if not text or not src:\n",
    "                fw.write(json.dumps({\"idx\": idx, \"ok\": False, \"error\": \"Missing text/source\", \"record\": rec}, ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            # === 关键：相对路径按 JSONL 目录解析，并保持使用解析后的 src_path ===\n",
    "            raw_src_path = Path(src)\n",
    "            src_path = raw_src_path if raw_src_path.is_absolute() else (base_dir / raw_src_path)\n",
    "            src_path = src_path.resolve()\n",
    "\n",
    "            if not src_path.exists():\n",
    "                fw.write(json.dumps({\n",
    "                    \"idx\": idx, \"ok\": False,\n",
    "                    \"error\": f\"Image not found\",\n",
    "                    \"source_in_json\": str(raw_src_path),\n",
    "                    \"resolved_source\": str(src_path)\n",
    "                }, ensure_ascii=False) + \"\\n\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with Image.open(src_path) as im:\n",
    "                    cond_img = im.convert(\"RGB\").resize((width, height))\n",
    "\n",
    "                out = pipeline(\n",
    "                    prompt=text,\n",
    "                    condition_image=cond_img,\n",
    "                    height=height,\n",
    "                    width=width,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    num_inference_steps=num_steps,\n",
    "                    max_sequence_length=max_sequence_length,\n",
    "                )\n",
    "                img = out.images[0]\n",
    "\n",
    "                sample_id = str(rec.get(\"id\", src_path.stem))\n",
    "                save_path = output_dir / f\"{sample_id}_generated.png\"\n",
    "\n",
    "                img.save(save_path)\n",
    "\n",
    "                relative_output = save_path.relative_to(output_dir)\n",
    "                result_rec = {\n",
    "                    **rec,\n",
    "                    \"idx\": idx,\n",
    "                    \"ok\": True,\n",
    "                    \"resolved_source\": str(src_path),\n",
    "                    \"output\": str(relative_output)\n",
    "                }\n",
    "                fw.write(json.dumps(result_rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                result_rec = {\n",
    "                    **rec,\n",
    "                    \"idx\": idx,\n",
    "                    \"ok\": False,\n",
    "                    \"resolved_source\": str(src_path),\n",
    "                    \"error\": f\"{type(e).__name__}: {str(e)}\"\n",
    "                }\n",
    "                fw.write(json.dumps(result_rec, ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39e80ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.52it/s]it/s]\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:01<00:01,  2.69it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading pretrain LoRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:20<00:00,  1.00s/it]?, ?it/s]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.04it/s]3:12:53, 22.47s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.04it/s]2:59:24, 20.94s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]2:55:03, 20.47s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]2:52:12, 20.18s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]2:50:43, 20.05s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]2:49:33, 19.95s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]2:48:45, 19.89s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]2:48:01, 19.84s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]2:47:29, 19.82s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:46:55, 19.79s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:46:27, 19.78s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:46:03, 19.77s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:45:44, 19.77s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:45:18, 19.76s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:44:55, 19.75s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:44:44, 19.77s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:44:21, 19.76s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:44:06, 19.77s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:43:41, 19.76s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:43:29, 19.78s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:43:02, 19.76s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:42:37, 19.75s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:42:22, 19.76s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:42:11, 19.78s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:41:49, 19.77s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:41:34, 19.78s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:41:10, 19.78s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:40:46, 19.77s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:40:33, 19.78s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:40:10, 19.78s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.05it/s]<2:39:57, 19.79s/it]\n",
      " 15%|█▌        | 3/20 [00:03<00:19,  1.17s/it]5<2:39:40, 19.79s/it]\n",
      "Batch Inference:   6%|▌         | 32/516 [10:39<2:41:08, 19.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m cfg = CONFIG\n\u001b[32m      2\u001b[39m pipe = load_pipeline(cfg[\u001b[33m\"\u001b[39m\u001b[33mmodel_path\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mlora_path\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mrun_batch_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjsonl_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjsonl_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwidth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mguidance_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_steps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_sequence_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mrun_batch_inference\u001b[39m\u001b[34m(pipeline, jsonl_path, output_dir, height, width, guidance_scale, num_steps, max_sequence_length, seed, device)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Image.open(src_path) \u001b[38;5;28;01mas\u001b[39;00m im:\n\u001b[32m     93\u001b[39m     cond_img = im.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m).resize((width, height))\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m out = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcondition_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcond_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m img = out.images[\u001b[32m0\u001b[39m]\n\u001b[32m    106\u001b[39m sample_id = \u001b[38;5;28mstr\u001b[39m(rec.get(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m, src_path.stem))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhotoDoodle/src/pipeline_pe_clone.py:681\u001b[39m, in \u001b[36mFluxPipeline.__call__\u001b[39m\u001b[34m(self, prompt, prompt_2, height, width, num_inference_steps, timesteps, guidance_scale, num_images_per_prompt, generator, latents, prompt_embeds, pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length, condition_image)\u001b[39m\n\u001b[32m    679\u001b[39m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[32m    680\u001b[39m timestep = t.expand(latents.shape[\u001b[32m0\u001b[39m]).to(latents.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 1 4096 64\u001b[39;49;00m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtxt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatent_image_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[32m    694\u001b[39m latents_dtype = latents.dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/diffusers/models/transformers/transformer_flux.py:565\u001b[39m, in \u001b[36mFluxTransformer2DModel.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, img_ids, txt_ids, guidance, joint_attention_kwargs, controlnet_block_samples, controlnet_single_block_samples, return_dict, controlnet_blocks_repeat)\u001b[39m\n\u001b[32m    556\u001b[39m     hidden_states = torch.utils.checkpoint.checkpoint(\n\u001b[32m    557\u001b[39m         create_custom_forward(block),\n\u001b[32m    558\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    561\u001b[39m         **ckpt_kwargs,\n\u001b[32m    562\u001b[39m     )\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     hidden_states = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[38;5;66;03m# controlnet residual\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m controlnet_single_block_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/diffusers/models/transformers/transformer_flux.py:97\u001b[39m, in \u001b[36mFluxSingleTransformerBlock.forward\u001b[39m\u001b[34m(self, hidden_states, temb, image_rotary_emb, joint_attention_kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m mlp_hidden_states = \u001b[38;5;28mself\u001b[39m.act_mlp(\u001b[38;5;28mself\u001b[39m.proj_mlp(norm_hidden_states))\n\u001b[32m     96\u001b[39m joint_attention_kwargs = joint_attention_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m hidden_states = torch.cat([attn_output, mlp_hidden_states], dim=\u001b[32m2\u001b[39m)\n\u001b[32m    104\u001b[39m gate = gate.unsqueeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/diffusers/models/attention_processor.py:588\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m     logger.warning(\n\u001b[32m    584\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.processor.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    585\u001b[39m     )\n\u001b[32m    586\u001b[39m cross_attention_kwargs = {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/diffusers/models/attention_processor.py:2319\u001b[39m, in \u001b[36mFluxAttnProcessor2_0.__call__\u001b[39m\u001b[34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, image_rotary_emb)\u001b[39m\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_rotary_emb\n\u001b[32m   2318\u001b[39m     query = apply_rotary_emb(query, image_rotary_emb)\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m     key = \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2321\u001b[39m hidden_states = F.scaled_dot_product_attention(\n\u001b[32m   2322\u001b[39m     query, key, value, attn_mask=attention_mask, dropout_p=\u001b[32m0.0\u001b[39m, is_causal=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2323\u001b[39m )\n\u001b[32m   2324\u001b[39m hidden_states = hidden_states.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).reshape(batch_size, -\u001b[32m1\u001b[39m, attn.heads * head_dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_lock_env/lib/python3.11/site-packages/diffusers/models/embeddings.py:1208\u001b[39m, in \u001b[36mapply_rotary_emb\u001b[39m\u001b[34m(x, freqs_cis, use_real, use_real_unbind_dim)\u001b[39m\n\u001b[32m   1205\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1206\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`use_real_unbind_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_real_unbind_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` but should be -1 or -2.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m     out = \u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_rotated\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1212\u001b[39m     \u001b[38;5;66;03m# used for lumina\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cfg = CONFIG\n",
    "pipe = load_pipeline(cfg[\"model_path\"], cfg[\"lora_path\"], cfg[\"dtype\"], cfg[\"device\"])\n",
    "run_batch_inference(\n",
    "    pipeline=pipe,\n",
    "    jsonl_path=cfg[\"jsonl_path\"],\n",
    "    output_dir=cfg[\"output_dir\"],\n",
    "    height=cfg[\"height\"],\n",
    "    width=cfg[\"width\"],\n",
    "    guidance_scale=cfg[\"guidance_scale\"],\n",
    "    num_steps=cfg[\"num_steps\"],\n",
    "    max_sequence_length=cfg[\"max_sequence_length\"],\n",
    "    seed=cfg[\"seed\"],\n",
    "    device=cfg[\"device\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_lock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
